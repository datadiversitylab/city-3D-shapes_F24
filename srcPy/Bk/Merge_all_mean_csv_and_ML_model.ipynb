{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Q2zzzw2IXOUm"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oL4jWvobs7wR",
    "outputId": "daa29e27-16a9-45f5-8d67-06343ad50381"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/Citywise Means 8 .csv\n",
      "/content/Citywise Means 4 .csv\n",
      "/content/Citywise Means 0 .csv\n",
      "/content/Citywise Means 3 .csv\n",
      "/content/Citywise Means 2 .csv\n",
      "/content/Citywise Means 7 .csv\n",
      "/content/Citywise Means 10 .csv\n",
      "/content/Citywise Means 5 .csv\n",
      "/content/Citywise Means 9 .csv\n",
      "/content/Citywise Means 6 .csv\n",
      "/content/Citywise Means 1 .csv\n",
      "All records have been appended and saved to 'combined_data.csv'\n",
      "Records with 'NA' values have been removed and saved to 'cleaned_combined_data.csv'\n"
     ]
    }
   ],
   "source": [
    "# Define the directory containing the files\n",
    "directory =  '../data/11_Output_R/'  #'/content/'\n",
    "outdir ='../data/12_Output_python/'\n",
    "\n",
    "filelist = glob.glob(directory+\"Citywise Means*.csv\")\n",
    "# Initialize an empty DataFrame\n",
    "ML_data = pd.DataFrame()\n",
    "\n",
    "first = True\n",
    "# Loop through all Citywise Means csv files in the directory\n",
    "for fl in filelist:  #os.listdir(directory):\n",
    "    #if fl.startswith('Citywise Means') and fl.endswith('.csv'):\n",
    "    flname = os.path.join(directory, fl)\n",
    "    print(flname)\n",
    "\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df1 = pd.read_csv(flname)\n",
    "\n",
    "    # merge df1 to ML_data\n",
    "    if (first):\n",
    "        ML_data = df1\n",
    "        first = False\n",
    "    else:\n",
    "        ML_data = pd.concat([ML_data, df1], ignore_index=True)\n",
    "\n",
    "#save to csv\n",
    "ML_data.to_csv( outdir+'UHI_NDVI_Swater_Means_data.csv', index=False)\n",
    "\n",
    "# Without 'NA values'\n",
    "ML_data_cleaned = ML_data.dropna()\n",
    "\n",
    "ML_data_cleaned.to_csv( outdir+'cleaned_UHI_NDVI_Swater_Means_data.csv', index=False)\n",
    "\n",
    "print(\"All records have been appended and saved to 'UHI_NDVI_Swater_Means_data.csv'\")\n",
    "print(\"Records with 'NA' values have been removed and saved to 'cleaned_UHI_NDVI_Swater_Means_data.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X41nsNcNDx-s",
    "outputId": "a09d4935-1ff8-4646-afcd-fa9af068ee0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0              City    mean_ndvi  mean_surface_water  \\\n",
      "0              1           Seattle  5885.997568            2.921069   \n",
      "1              2           Sechura  1865.855202            1.736842   \n",
      "2              3             Sefra  1704.591465                 NaN   \n",
      "3              4           Segezha  3214.220035            2.626866   \n",
      "4              5             Segou  2033.636927            2.696721   \n",
      "...          ...               ...          ...                 ...   \n",
      "5977         593  Clermont-Ferrand  5367.516250            2.052632   \n",
      "5978         594         Cleveland  4225.224380            2.798190   \n",
      "5979         595             Cliza     0.000000            1.500000   \n",
      "5980         596         Cloncurry  2479.763975            2.000000   \n",
      "5981         597            Clovis  3143.816084            1.200000   \n",
      "\n",
      "      mean_uhi_DayNight  mean_uhi_daytime  mean_uhi_Nighttime      Continent  \\\n",
      "0              0.761848          0.577815            0.184033  NORTH AMERICA   \n",
      "1                   NaN               NaN                 NaN  SOUTH AMERICA   \n",
      "2             -0.782306         -0.953312            0.171006         AFRICA   \n",
      "3              0.546307          0.589458           -0.043151         EUROPE   \n",
      "4              2.769901          0.743021            2.026880         AFRICA   \n",
      "...                 ...               ...                 ...            ...   \n",
      "5977           1.006390          0.012941            0.993449         EUROPE   \n",
      "5978           0.984145          0.702462            0.281683  NORTH AMERICA   \n",
      "5979                NaN               NaN                 NaN  SOUTH AMERICA   \n",
      "5980                NaN               NaN                 NaN      AUSTRALIA   \n",
      "5981           1.396374          0.535138            0.861236  NORTH AMERICA   \n",
      "\n",
      "          Class  Skewness  Dip_Statistic  Mean_Height  \n",
      "0     Hourglass  2.489935       0.000000     1.601108  \n",
      "1     Hourglass  0.591126       0.000030     1.336148  \n",
      "2     Hourglass  0.694696       0.000000     1.016515  \n",
      "3       Pyramid  1.381091       1.000000     0.598886  \n",
      "4     Hourglass -0.331938       0.000000     2.149867  \n",
      "...         ...       ...            ...          ...  \n",
      "5977    Pyramid  1.412190       0.638883     1.200756  \n",
      "5978  Hourglass  3.087886       0.000936     1.112245  \n",
      "5979    Pyramid  3.408363       0.996836     0.382762  \n",
      "5980    Pyramid  0.867233       0.912263     0.880249  \n",
      "5981    Pyramid  0.646432       0.101141     1.139136  \n",
      "\n",
      "[5982 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "directory =   ''  #'/content/'\n",
    "\n",
    "file_path1 = os.path.join(directory, 'UHI_NDVI_Swater_Means_data.csv')\n",
    "file_path2 = os.path.join(directory, 'combined_summary_City_Shape.csv')\n",
    "file_path3 = os.path.join(directory, 'City_coordinates.csv')\n",
    "df_means = pd.read_csv(file_path1)\n",
    "df_city_shape = pd.read_csv(file_path2)\n",
    "df_cooord = pd.read_csv(file_path3)\n",
    "\n",
    "df_means.rename(columns={'city': 'City'}, inplace=True)\n",
    "all_data = pd.merge(df_means, df_city_shape , on='City' )\n",
    "\n",
    "# Saveto CSV\n",
    "all_data.to_csv('ML_input_data.csv', index=False)\n",
    "print( all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "chOGQrC5YQjf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ulnfkXL04qR4"
   },
   "outputs": [],
   "source": [
    "# EDA\n",
    "UHI_data.head()\n",
    "UHI_data.describe()\n",
    "UHI_data.describe(include = 'all')\n",
    "UHI_data[['Direction']].value_counts()\n",
    "UHI_data.isna().sum()\n",
    "sns.pairplot(UHI_data,hue='Direction')\n",
    "\n",
    "endog=(UHI_data[['Direction']]=='Up').astype('int64')\n",
    "exog=sm.add_constant(UHI_data.drop(columns=['Direction','Year','Today']))\n",
    "logit_mod=sm.Logit(endog,exog)\n",
    "logit_res=logit_mod.fit()\n",
    "logit_res.summary()\n",
    "logit_res.pred_table()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d34Hze2ohQaX"
   },
   "outputs": [],
   "source": [
    "matrix = pd.DataFrame(logit_res.pred_table(), columns = [\"Down\", \"Up\"], index = [\"Down\", \"Up\"])\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.heatmap(matrix, annot = True, cbar = False, ax = ax, fmt = \"g\", square = True, annot_kws = {\"fontsize\": \"x-large\"})\n",
    "ax.set(xlabel = \"predicted label\", ylabel = \"true label\");\n",
    "\n",
    "TN = logit_res.pred_table()[0,0] # 54\n",
    "TP = logit_res.pred_table()[1,1] #557\n",
    "FN = logit_res.pred_table()[1,0] #48\n",
    "FP = logit_res.pred_table()[0,1] #557\n",
    "\n",
    "Accuracy = (TP+TN)/(TP+TN+FP+FN)\n",
    "Recall = TP/(TP+FN)\n",
    "Precision = TP/(TP+FP)\n",
    "\n",
    "[Accuracy,Recall,Precision]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l8EIbHYI4lMc"
   },
   "outputs": [],
   "source": [
    "# Build ML model\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
